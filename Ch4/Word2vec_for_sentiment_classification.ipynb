{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkpathak\\AppData\\Local\\Continuum\\anaconda2\\envs\\py34t\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# load the google word2vec model this takes time and memory and hence it's wise to do it first and them move to other\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'Word2vec\\\\GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkpathak\\AppData\\Local\\Continuum\\anaconda2\\envs\\py34t\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_data = pd.DataFrame(columns = (\"Text\", \"Class\")) \n",
    "for cla in glob.glob(\"C:/Users/inkpathak/Desktop/Anuj Gupta/Word2vec/txt_sentoken/*\"): # Here the folder name is data set in which there are two sub-folder \"Spam\" & \"Ham\"\n",
    "    clas = cla.split(os.sep)[1]    # We are splitting the folder names as class using OS-Seperator and taking the 2nd item in the list\n",
    "    for file in glob.glob(cla + \"/*.txt\"): # Here we are deep diving in each of the folder and reading the text files one by one\n",
    "        text = open(file, \"r\", encoding = \"ISO-8859-1\").read() # Reading the file , for Windows generally we need to mention the encoding \n",
    "        text = \" \".join(text.split(\"\\n\")) # Splitting the text files and rejoining into a single text\n",
    "        senti_data = senti_data.append(pd.Series([text, clas], index = [\"Text\", \"Class\"]), ignore_index = True) # continious append to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Class\n",
       "0  plot : two teen couples go to a church party ,...   neg\n",
       "1  the happy bastard's quick movie review  damn t...   neg\n",
       "2  it is movies like these that make a jaded movi...   neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...   neg\n",
       "4  synopsis : a mentally unstable man undergoing ...   neg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "senti_data['Class'] = senti_data.Class.map({'neg':0, 'pos':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class\n",
       "0  plot : two teen couples go to a church party ,...      0\n",
       "1  the happy bastard's quick movie review  damn t...      0\n",
       "2  it is movies like these that make a jaded movi...      0\n",
       "3   \" quest for camelot \" is warner bros . ' firs...      0\n",
       "4  synopsis : a mentally unstable man undergoing ...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = \" \".join([i.replace('*', '') for i in doc.lower().split()])\n",
    "    doc = \" \".join([i.replace(':', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('.', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('=', '') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('/', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace(')', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('(', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('\"', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('-', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('_', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i for i in doc.split() if not i.isdigit()])\n",
    "    doc = \" \".join([i for i in doc.split() if i.isalpha()])\n",
    "    doc = \" \".join([i for i in doc.split() if i not in stop])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>plot two teen couples go church party drink dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>0</td>\n",
       "      <td>happy quick movie review damn bug got head sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>movies like make jaded movie viewer thankful i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>quest camelot warner bros first feature length...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>synopsis mentally unstable man undergoing psyc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0  plot : two teen couples go to a church party ,...      0   \n",
       "1  the happy bastard's quick movie review  damn t...      0   \n",
       "2  it is movies like these that make a jaded movi...      0   \n",
       "3   \" quest for camelot \" is warner bros . ' firs...      0   \n",
       "4  synopsis : a mentally unstable man undergoing ...      0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  plot two teen couples go church party drink dr...  \n",
       "1  happy quick movie review damn bug got head sta...  \n",
       "2  movies like make jaded movie viewer thankful i...  \n",
       "3  quest camelot warner bros first feature length...  \n",
       "4  synopsis mentally unstable man undergoing psyc...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_clear = [clean(doc) for doc in senti_data['Text']]\n",
    "senti_data['clean_text']=review_clear\n",
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build sentense vecto for training set by using the total value of all word vectors in the Clean text column\n",
    "\n",
    "def buildSentenceVector(text):\n",
    "    sent_vec = np.zeros(300).reshape((1, 300))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            sent_vec += model[word].reshape((1, 300))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    #if count != 0:\n",
    "    #    sent_vec /= count\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_vec = [buildSentenceVector(doc) for doc in senti_data['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti_data['sentense_vector']=review_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentense_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>plot two teen couples go church party drink dr...</td>\n",
       "      <td>[[-293.95513916015625, 192.2789306640625, -4.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>0</td>\n",
       "      <td>happy quick movie review damn bug got head sta...</td>\n",
       "      <td>[[-112.78167724609375, 65.20263671875, 0.33660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>movies like make jaded movie viewer thankful i...</td>\n",
       "      <td>[[-242.486083984375, 163.73614501953125, 0.711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>quest camelot warner bros first feature length...</td>\n",
       "      <td>[[-250.0531005859375, 168.8553466796875, -4.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>synopsis mentally unstable man undergoing psyc...</td>\n",
       "      <td>[[-378.0009765625, 267.97918701171875, 1.84680...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0  plot : two teen couples go to a church party ,...      0   \n",
       "1  the happy bastard's quick movie review  damn t...      0   \n",
       "2  it is movies like these that make a jaded movi...      0   \n",
       "3   \" quest for camelot \" is warner bros . ' firs...      0   \n",
       "4  synopsis : a mentally unstable man undergoing ...      0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  plot two teen couples go church party drink dr...   \n",
       "1  happy quick movie review damn bug got head sta...   \n",
       "2  movies like make jaded movie viewer thankful i...   \n",
       "3  quest camelot warner bros first feature length...   \n",
       "4  synopsis mentally unstable man undergoing psyc...   \n",
       "\n",
       "                                     sentense_vector  \n",
       "0  [[-293.95513916015625, 192.2789306640625, -4.9...  \n",
       "1  [[-112.78167724609375, 65.20263671875, 0.33660...  \n",
       "2  [[-242.486083984375, 163.73614501953125, 0.711...  \n",
       "3  [[-250.0531005859375, 168.8553466796875, -4.26...  \n",
       "4  [[-378.0009765625, 267.97918701171875, 1.84680...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_vec1 = [i[0] for i in review_vec] # changing from list of list to single list\n",
    "review_vec2 = np.array(review_vec1) # Changing from single list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vec2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_df = pd.DataFrame(review_vec2) # changinf the array to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-293.955139</td>\n",
       "      <td>192.278931</td>\n",
       "      <td>-4.972412</td>\n",
       "      <td>246.479492</td>\n",
       "      <td>-76.376129</td>\n",
       "      <td>52.847504</td>\n",
       "      <td>-149.930115</td>\n",
       "      <td>-68.620850</td>\n",
       "      <td>-81.728027</td>\n",
       "      <td>26.597717</td>\n",
       "      <td>...</td>\n",
       "      <td>116.883545</td>\n",
       "      <td>-33.414062</td>\n",
       "      <td>-166.789688</td>\n",
       "      <td>156.874390</td>\n",
       "      <td>-55.962769</td>\n",
       "      <td>-263.559921</td>\n",
       "      <td>-162.363647</td>\n",
       "      <td>-40.535278</td>\n",
       "      <td>-190.335449</td>\n",
       "      <td>264.554199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-112.781677</td>\n",
       "      <td>65.202637</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>92.041504</td>\n",
       "      <td>-27.368500</td>\n",
       "      <td>18.075989</td>\n",
       "      <td>-55.470612</td>\n",
       "      <td>-36.053345</td>\n",
       "      <td>-30.213440</td>\n",
       "      <td>6.324829</td>\n",
       "      <td>...</td>\n",
       "      <td>41.426025</td>\n",
       "      <td>2.624512</td>\n",
       "      <td>-56.628387</td>\n",
       "      <td>61.097534</td>\n",
       "      <td>-24.956787</td>\n",
       "      <td>-102.484375</td>\n",
       "      <td>-68.589966</td>\n",
       "      <td>-16.991699</td>\n",
       "      <td>-71.407837</td>\n",
       "      <td>101.894714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-242.486084</td>\n",
       "      <td>163.736145</td>\n",
       "      <td>0.711853</td>\n",
       "      <td>198.322754</td>\n",
       "      <td>-68.430664</td>\n",
       "      <td>43.944977</td>\n",
       "      <td>-130.381378</td>\n",
       "      <td>-61.356079</td>\n",
       "      <td>-73.543579</td>\n",
       "      <td>25.141663</td>\n",
       "      <td>...</td>\n",
       "      <td>109.481201</td>\n",
       "      <td>-21.978760</td>\n",
       "      <td>-149.334961</td>\n",
       "      <td>124.054443</td>\n",
       "      <td>-40.955688</td>\n",
       "      <td>-225.843277</td>\n",
       "      <td>-138.615723</td>\n",
       "      <td>-27.549072</td>\n",
       "      <td>-161.180542</td>\n",
       "      <td>220.218445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-250.053101</td>\n",
       "      <td>168.855347</td>\n",
       "      <td>-4.266174</td>\n",
       "      <td>195.756836</td>\n",
       "      <td>-72.587067</td>\n",
       "      <td>44.229919</td>\n",
       "      <td>-127.833618</td>\n",
       "      <td>-64.556030</td>\n",
       "      <td>-76.520386</td>\n",
       "      <td>25.055115</td>\n",
       "      <td>...</td>\n",
       "      <td>100.104736</td>\n",
       "      <td>-21.081909</td>\n",
       "      <td>-139.455933</td>\n",
       "      <td>126.582520</td>\n",
       "      <td>-42.903931</td>\n",
       "      <td>-221.102890</td>\n",
       "      <td>-135.423828</td>\n",
       "      <td>-37.146851</td>\n",
       "      <td>-160.887817</td>\n",
       "      <td>230.279663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-378.000977</td>\n",
       "      <td>267.979187</td>\n",
       "      <td>1.846802</td>\n",
       "      <td>302.576904</td>\n",
       "      <td>-102.295349</td>\n",
       "      <td>59.879395</td>\n",
       "      <td>-188.510895</td>\n",
       "      <td>-105.857422</td>\n",
       "      <td>-103.271912</td>\n",
       "      <td>29.568604</td>\n",
       "      <td>...</td>\n",
       "      <td>149.544434</td>\n",
       "      <td>-43.593750</td>\n",
       "      <td>-232.855728</td>\n",
       "      <td>211.853516</td>\n",
       "      <td>-71.249023</td>\n",
       "      <td>-353.665085</td>\n",
       "      <td>-206.748779</td>\n",
       "      <td>-61.833618</td>\n",
       "      <td>-227.511963</td>\n",
       "      <td>374.038635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1         2           3           4          5    \\\n",
       "0 -293.955139  192.278931 -4.972412  246.479492  -76.376129  52.847504   \n",
       "1 -112.781677   65.202637  0.336609   92.041504  -27.368500  18.075989   \n",
       "2 -242.486084  163.736145  0.711853  198.322754  -68.430664  43.944977   \n",
       "3 -250.053101  168.855347 -4.266174  195.756836  -72.587067  44.229919   \n",
       "4 -378.000977  267.979187  1.846802  302.576904 -102.295349  59.879395   \n",
       "\n",
       "          6           7           8          9       ...             290  \\\n",
       "0 -149.930115  -68.620850  -81.728027  26.597717     ...      116.883545   \n",
       "1  -55.470612  -36.053345  -30.213440   6.324829     ...       41.426025   \n",
       "2 -130.381378  -61.356079  -73.543579  25.141663     ...      109.481201   \n",
       "3 -127.833618  -64.556030  -76.520386  25.055115     ...      100.104736   \n",
       "4 -188.510895 -105.857422 -103.271912  29.568604     ...      149.544434   \n",
       "\n",
       "         291         292         293        294         295         296  \\\n",
       "0 -33.414062 -166.789688  156.874390 -55.962769 -263.559921 -162.363647   \n",
       "1   2.624512  -56.628387   61.097534 -24.956787 -102.484375  -68.589966   \n",
       "2 -21.978760 -149.334961  124.054443 -40.955688 -225.843277 -138.615723   \n",
       "3 -21.081909 -139.455933  126.582520 -42.903931 -221.102890 -135.423828   \n",
       "4 -43.593750 -232.855728  211.853516 -71.249023 -353.665085 -206.748779   \n",
       "\n",
       "         297         298         299  \n",
       "0 -40.535278 -190.335449  264.554199  \n",
       "1 -16.991699  -71.407837  101.894714  \n",
       "2 -27.549072 -161.180542  220.218445  \n",
       "3 -37.146851 -160.887817  230.279663  \n",
       "4 -61.833618 -227.511963  374.038635  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_df[\"sentiment\"] = senti_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-293.955139</td>\n",
       "      <td>192.278931</td>\n",
       "      <td>-4.972412</td>\n",
       "      <td>246.479492</td>\n",
       "      <td>-76.376129</td>\n",
       "      <td>52.847504</td>\n",
       "      <td>-149.930115</td>\n",
       "      <td>-68.620850</td>\n",
       "      <td>-81.728027</td>\n",
       "      <td>26.597717</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.414062</td>\n",
       "      <td>-166.789688</td>\n",
       "      <td>156.874390</td>\n",
       "      <td>-55.962769</td>\n",
       "      <td>-263.559921</td>\n",
       "      <td>-162.363647</td>\n",
       "      <td>-40.535278</td>\n",
       "      <td>-190.335449</td>\n",
       "      <td>264.554199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-112.781677</td>\n",
       "      <td>65.202637</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>92.041504</td>\n",
       "      <td>-27.368500</td>\n",
       "      <td>18.075989</td>\n",
       "      <td>-55.470612</td>\n",
       "      <td>-36.053345</td>\n",
       "      <td>-30.213440</td>\n",
       "      <td>6.324829</td>\n",
       "      <td>...</td>\n",
       "      <td>2.624512</td>\n",
       "      <td>-56.628387</td>\n",
       "      <td>61.097534</td>\n",
       "      <td>-24.956787</td>\n",
       "      <td>-102.484375</td>\n",
       "      <td>-68.589966</td>\n",
       "      <td>-16.991699</td>\n",
       "      <td>-71.407837</td>\n",
       "      <td>101.894714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-242.486084</td>\n",
       "      <td>163.736145</td>\n",
       "      <td>0.711853</td>\n",
       "      <td>198.322754</td>\n",
       "      <td>-68.430664</td>\n",
       "      <td>43.944977</td>\n",
       "      <td>-130.381378</td>\n",
       "      <td>-61.356079</td>\n",
       "      <td>-73.543579</td>\n",
       "      <td>25.141663</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.978760</td>\n",
       "      <td>-149.334961</td>\n",
       "      <td>124.054443</td>\n",
       "      <td>-40.955688</td>\n",
       "      <td>-225.843277</td>\n",
       "      <td>-138.615723</td>\n",
       "      <td>-27.549072</td>\n",
       "      <td>-161.180542</td>\n",
       "      <td>220.218445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-250.053101</td>\n",
       "      <td>168.855347</td>\n",
       "      <td>-4.266174</td>\n",
       "      <td>195.756836</td>\n",
       "      <td>-72.587067</td>\n",
       "      <td>44.229919</td>\n",
       "      <td>-127.833618</td>\n",
       "      <td>-64.556030</td>\n",
       "      <td>-76.520386</td>\n",
       "      <td>25.055115</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.081909</td>\n",
       "      <td>-139.455933</td>\n",
       "      <td>126.582520</td>\n",
       "      <td>-42.903931</td>\n",
       "      <td>-221.102890</td>\n",
       "      <td>-135.423828</td>\n",
       "      <td>-37.146851</td>\n",
       "      <td>-160.887817</td>\n",
       "      <td>230.279663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-378.000977</td>\n",
       "      <td>267.979187</td>\n",
       "      <td>1.846802</td>\n",
       "      <td>302.576904</td>\n",
       "      <td>-102.295349</td>\n",
       "      <td>59.879395</td>\n",
       "      <td>-188.510895</td>\n",
       "      <td>-105.857422</td>\n",
       "      <td>-103.271912</td>\n",
       "      <td>29.568604</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.593750</td>\n",
       "      <td>-232.855728</td>\n",
       "      <td>211.853516</td>\n",
       "      <td>-71.249023</td>\n",
       "      <td>-353.665085</td>\n",
       "      <td>-206.748779</td>\n",
       "      <td>-61.833618</td>\n",
       "      <td>-227.511963</td>\n",
       "      <td>374.038635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1         2           3           4          5  \\\n",
       "0 -293.955139  192.278931 -4.972412  246.479492  -76.376129  52.847504   \n",
       "1 -112.781677   65.202637  0.336609   92.041504  -27.368500  18.075989   \n",
       "2 -242.486084  163.736145  0.711853  198.322754  -68.430664  43.944977   \n",
       "3 -250.053101  168.855347 -4.266174  195.756836  -72.587067  44.229919   \n",
       "4 -378.000977  267.979187  1.846802  302.576904 -102.295349  59.879395   \n",
       "\n",
       "            6           7           8          9    ...            291  \\\n",
       "0 -149.930115  -68.620850  -81.728027  26.597717    ...     -33.414062   \n",
       "1  -55.470612  -36.053345  -30.213440   6.324829    ...       2.624512   \n",
       "2 -130.381378  -61.356079  -73.543579  25.141663    ...     -21.978760   \n",
       "3 -127.833618  -64.556030  -76.520386  25.055115    ...     -21.081909   \n",
       "4 -188.510895 -105.857422 -103.271912  29.568604    ...     -43.593750   \n",
       "\n",
       "          292         293        294         295         296        297  \\\n",
       "0 -166.789688  156.874390 -55.962769 -263.559921 -162.363647 -40.535278   \n",
       "1  -56.628387   61.097534 -24.956787 -102.484375  -68.589966 -16.991699   \n",
       "2 -149.334961  124.054443 -40.955688 -225.843277 -138.615723 -27.549072   \n",
       "3 -139.455933  126.582520 -42.903931 -221.102890 -135.423828 -37.146851   \n",
       "4 -232.855728  211.853516 -71.249023 -353.665085 -206.748779 -61.833618   \n",
       "\n",
       "          298         299  sentiment  \n",
       "0 -190.335449  264.554199          0  \n",
       "1  -71.407837  101.894714          0  \n",
       "2 -161.180542  220.218445          0  \n",
       "3 -160.887817  230.279663          0  \n",
       "4 -227.511963  374.038635          0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 300)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "X = review_df.iloc[:,0:300]\n",
    "y = review_df.sentiment\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scale X here by each row\n",
    "from sklearn.preprocessing import scale\n",
    "X= scale(X, axis=1, with_mean=True, with_std=True, copy=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 300)\n",
      "(500, 300)\n",
      "(1500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2. instantiate a logistic regression model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 622 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.588"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165,  90],\n",
       "       [116, 129]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "# Checking how many words are there in google Word to vec \n",
    "word2vec_vocab = model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synopsis',\n",
       " 'mentally',\n",
       " 'unstable',\n",
       " 'man',\n",
       " 'undergoing',\n",
       " 'psychotherapy',\n",
       " 'saves',\n",
       " 'boy',\n",
       " 'potentially',\n",
       " 'fatal',\n",
       " 'accident',\n",
       " 'falls',\n",
       " 'love',\n",
       " 'mother',\n",
       " 'fledgling',\n",
       " 'restauranteur',\n",
       " 'unsuccessfully',\n",
       " 'attempting',\n",
       " 'gain',\n",
       " 'favor',\n",
       " 'takes',\n",
       " 'pictures',\n",
       " 'kills',\n",
       " 'number',\n",
       " 'people',\n",
       " 'way',\n",
       " 'comments',\n",
       " 'stalked',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'seemingly',\n",
       " 'endless',\n",
       " 'string',\n",
       " 'spurned',\n",
       " 'psychos',\n",
       " 'getting',\n",
       " 'revenge',\n",
       " 'type',\n",
       " 'movies',\n",
       " 'stable',\n",
       " 'category',\n",
       " 'film',\n",
       " 'industry',\n",
       " 'theatrical',\n",
       " 'direct',\n",
       " 'video',\n",
       " 'proliferation',\n",
       " 'may',\n",
       " 'due',\n",
       " 'part',\n",
       " 'fact',\n",
       " 'typically',\n",
       " 'inexpensive',\n",
       " 'produce',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'big',\n",
       " 'name',\n",
       " 'stars',\n",
       " 'serve',\n",
       " 'vehicles',\n",
       " 'flash',\n",
       " 'nudity',\n",
       " 'allowing',\n",
       " 'frequent',\n",
       " 'late',\n",
       " 'night',\n",
       " 'cable',\n",
       " 'television',\n",
       " 'stalked',\n",
       " 'wavers',\n",
       " 'slightly',\n",
       " 'norm',\n",
       " 'one',\n",
       " 'respect',\n",
       " 'psycho',\n",
       " 'never',\n",
       " 'actually',\n",
       " 'affair',\n",
       " 'contrary',\n",
       " 'rejected',\n",
       " 'rather',\n",
       " 'quickly',\n",
       " 'psycho',\n",
       " 'typically',\n",
       " 'ex',\n",
       " 'lover',\n",
       " 'ex',\n",
       " 'wife',\n",
       " 'ex',\n",
       " 'husband',\n",
       " 'stalked',\n",
       " 'another',\n",
       " 'redundant',\n",
       " 'entry',\n",
       " 'doomed',\n",
       " 'collect',\n",
       " 'dust',\n",
       " 'video',\n",
       " 'shelves',\n",
       " 'viewed',\n",
       " 'midnight',\n",
       " 'cable',\n",
       " 'stalked',\n",
       " 'provide',\n",
       " 'much',\n",
       " 'suspense',\n",
       " 'though',\n",
       " 'sets',\n",
       " 'interspersed',\n",
       " 'throughout',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'instance',\n",
       " 'serious',\n",
       " 'sounding',\n",
       " 'narrator',\n",
       " 'spouts',\n",
       " 'statistics',\n",
       " 'stalkers',\n",
       " 'ponders',\n",
       " 'may',\n",
       " 'cause',\n",
       " 'man',\n",
       " 'stalk',\n",
       " 'implicitly',\n",
       " 'implied',\n",
       " 'stalkers',\n",
       " 'men',\n",
       " 'pictures',\n",
       " 'boy',\n",
       " 'shown',\n",
       " 'screen',\n",
       " 'credits',\n",
       " 'snapshot',\n",
       " 'actor',\n",
       " 'jay',\n",
       " 'underwood',\n",
       " 'appears',\n",
       " 'narrator',\n",
       " 'states',\n",
       " 'story',\n",
       " 'daryl',\n",
       " 'gleason',\n",
       " 'tells',\n",
       " 'audience',\n",
       " 'stalker',\n",
       " 'course',\n",
       " 'really',\n",
       " 'story',\n",
       " 'restauranteur',\n",
       " 'brooke',\n",
       " 'daniels',\n",
       " 'movie',\n",
       " 'meant',\n",
       " 'daryl',\n",
       " 'called',\n",
       " 'stalker',\n",
       " 'stalked',\n",
       " 'okay',\n",
       " 'know',\n",
       " 'stalker',\n",
       " 'even',\n",
       " 'movie',\n",
       " 'starts',\n",
       " 'guesswork',\n",
       " 'required',\n",
       " 'stalked',\n",
       " 'proceeds',\n",
       " 'begins',\n",
       " 'obvious',\n",
       " 'obvious',\n",
       " 'obvious',\n",
       " 'opening',\n",
       " 'sequence',\n",
       " 'contrived',\n",
       " 'quite',\n",
       " 'bit',\n",
       " 'brings',\n",
       " 'daryl',\n",
       " 'brooke',\n",
       " 'victim',\n",
       " 'together',\n",
       " 'daryl',\n",
       " 'obsesses',\n",
       " 'brooke',\n",
       " 'follows',\n",
       " 'around',\n",
       " 'tries',\n",
       " 'woo',\n",
       " 'ultimately',\n",
       " 'rejected',\n",
       " 'plans',\n",
       " 'become',\n",
       " 'desperate',\n",
       " 'elaborate',\n",
       " 'plans',\n",
       " 'include',\n",
       " 'time',\n",
       " 'psycho',\n",
       " 'love',\n",
       " 'cliche',\n",
       " 'murdered',\n",
       " 'pet',\n",
       " 'reason',\n",
       " 'films',\n",
       " 'require',\n",
       " 'dead',\n",
       " 'pet',\n",
       " 'found',\n",
       " 'victim',\n",
       " 'stalked',\n",
       " 'stalked',\n",
       " 'exception',\n",
       " 'cat',\n",
       " 'time',\n",
       " 'found',\n",
       " 'shower',\n",
       " 'events',\n",
       " 'like',\n",
       " 'lead',\n",
       " 'inevitable',\n",
       " 'showdown',\n",
       " 'stalker',\n",
       " 'stalked',\n",
       " 'one',\n",
       " 'survives',\n",
       " 'guess',\n",
       " 'invariably',\n",
       " 'always',\n",
       " 'guess',\n",
       " 'conclusion',\n",
       " 'turkey',\n",
       " 'cast',\n",
       " 'uniformly',\n",
       " 'adequate',\n",
       " 'anything',\n",
       " 'write',\n",
       " 'home',\n",
       " 'also',\n",
       " 'bad',\n",
       " 'either',\n",
       " 'jay',\n",
       " 'underwood',\n",
       " 'stalker',\n",
       " 'turns',\n",
       " 'toward',\n",
       " 'melodrama',\n",
       " 'bit',\n",
       " 'much',\n",
       " 'overdoes',\n",
       " 'words',\n",
       " 'still',\n",
       " 'manages',\n",
       " 'creepy',\n",
       " 'enough',\n",
       " 'pass',\n",
       " 'type',\n",
       " 'stalker',\n",
       " 'story',\n",
       " 'demands',\n",
       " 'maryam',\n",
       " 'actor',\n",
       " 'close',\n",
       " 'star',\n",
       " 'played',\n",
       " 'bond',\n",
       " 'chick',\n",
       " 'living',\n",
       " 'daylights',\n",
       " 'equally',\n",
       " 'adequate',\n",
       " 'stalked',\n",
       " 'title',\n",
       " 'even',\n",
       " 'though',\n",
       " 'seems',\n",
       " 'ditzy',\n",
       " 'times',\n",
       " 'strong',\n",
       " 'independent',\n",
       " 'business',\n",
       " 'owner',\n",
       " 'brooke',\n",
       " 'needs',\n",
       " 'ditzy',\n",
       " 'however',\n",
       " 'plot',\n",
       " 'proceed',\n",
       " 'toward',\n",
       " 'end',\n",
       " 'example',\n",
       " 'brooke',\n",
       " 'suspicions',\n",
       " 'daryl',\n",
       " 'ensure',\n",
       " 'use',\n",
       " 'another',\n",
       " 'excuse',\n",
       " 'see',\n",
       " 'brooke',\n",
       " 'decides',\n",
       " 'return',\n",
       " 'toolbox',\n",
       " 'left',\n",
       " 'place',\n",
       " 'house',\n",
       " 'leave',\n",
       " 'toolbox',\n",
       " 'door',\n",
       " 'one',\n",
       " 'answers',\n",
       " 'course',\n",
       " 'tries',\n",
       " 'door',\n",
       " 'opens',\n",
       " 'wanders',\n",
       " 'around',\n",
       " 'house',\n",
       " 'daryl',\n",
       " 'returns',\n",
       " 'enters',\n",
       " 'house',\n",
       " 'course',\n",
       " 'heroine',\n",
       " 'danger',\n",
       " 'somehow',\n",
       " 'even',\n",
       " 'though',\n",
       " 'car',\n",
       " 'parked',\n",
       " 'front',\n",
       " 'house',\n",
       " 'right',\n",
       " 'front',\n",
       " 'door',\n",
       " 'daryl',\n",
       " 'oblivious',\n",
       " 'presence',\n",
       " 'inside',\n",
       " 'whole',\n",
       " 'episode',\n",
       " 'places',\n",
       " 'incredible',\n",
       " 'strain',\n",
       " 'suspension',\n",
       " 'disbelief',\n",
       " 'questions',\n",
       " 'validity',\n",
       " 'either',\n",
       " 'intelligence',\n",
       " 'stalked',\n",
       " 'receives',\n",
       " 'two',\n",
       " 'stars',\n",
       " 'even',\n",
       " 'though',\n",
       " 'highly',\n",
       " 'derivative',\n",
       " 'somewhat',\n",
       " 'boring',\n",
       " 'bad',\n",
       " 'cannot',\n",
       " 'watched',\n",
       " 'rated',\n",
       " 'r',\n",
       " 'mostly',\n",
       " 'several',\n",
       " 'murder',\n",
       " 'scenes',\n",
       " 'brief',\n",
       " 'nudity',\n",
       " 'strip',\n",
       " 'bar',\n",
       " 'offensive',\n",
       " 'many',\n",
       " 'thrillers',\n",
       " 'genre',\n",
       " 'mood',\n",
       " 'good',\n",
       " 'suspense',\n",
       " 'film',\n",
       " 'though',\n",
       " 'stake',\n",
       " 'something',\n",
       " 'else']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data[\"clean_text\"][4].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(senti_data.shape[0]):\n",
    "    kk=senti_data[\"clean_text\"][i].split()\n",
    "    word_list.append(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list = [item for sublist in word_list for item in sublist] unlisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689259"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_words = list(set(word_list))  #this will give unique list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38333\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31474\n"
     ]
    }
   ],
   "source": [
    "kk=0\n",
    "for word in unique_words:\n",
    "    try:\n",
    "        \n",
    "        kp= model[word]\n",
    "        kk +=1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(kk) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8210680092870373\n"
     ]
    }
   ],
   "source": [
    "print(kk/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
